{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a93d0b3d",
   "metadata": {},
   "source": [
    "# PLN: Fundamento de procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa290087",
   "metadata": {},
   "source": [
    "## Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b6bd584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gabod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gabod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gabod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\gabod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"punkt_tab\")  \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df336497",
   "metadata": {},
   "source": [
    "## Texto de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4bf0552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus original:\n",
      "1: El gato duerme en la cama\n",
      "2: Los perros juegan en el parque\n",
      "3: El perro no duerme en la cama, duerme en la sillón 2\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"El gato duerme en la cama\",\n",
    "    \"Los perros juegan en el parque\",\n",
    "    \"El perro no duerme en la cama, duerme en la sillón 2\"\n",
    "]\n",
    "\n",
    "print(\"Corpus original:\")\n",
    "for i, doc in enumerate(corpus):\n",
    "    print(f\"{i+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c85449",
   "metadata": {},
   "source": [
    "## Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8916e20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corpus normalizado:\n",
      "['el gato duerme en la cama', 'los perros juegan en el parque', 'el perro no duerme en la cama duerme en la sillón ']\n"
     ]
    }
   ],
   "source": [
    "def normalize(text):\n",
    "    text = text.lower() #todo a minúsculas\n",
    "    text = re.sub(r\"[^a-záéíóúüñ ]\", \"\", text) # eliminar signos de puntuación\n",
    "    text = re.sub(r\"/d+\", \"\", text) # eliminar números\n",
    "    return text\n",
    "\n",
    "normalized_corpus = [normalize(doc) for doc in corpus]\n",
    "print(\"\\nCorpus normalizado:\")\n",
    "print(normalized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986d4f9",
   "metadata": {},
   "source": [
    "## Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0da2222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens:\n",
      "2: ['el', 'gato', 'duerme', 'en', 'la', 'cama']\n",
      "3: ['los', 'perros', 'juegan', 'en', 'el', 'parque']\n",
      "4: ['el', 'perro', 'no', 'duerme', 'en', 'la', 'cama', 'duerme', 'en', 'la', 'sillón']\n"
     ]
    }
   ],
   "source": [
    "tokens =  [nltk.word_tokenize(doc, language=\"spanish\") for doc in normalized_corpus]\n",
    "print(\"\\nTokens:\")\n",
    "for i, tks in enumerate(tokens, 1):\n",
    "    print(f\"{i+1}: {tks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6599a540",
   "metadata": {},
   "source": [
    "## Eliminación de Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "446beb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens sin stopwords:\n",
      "2: ['gato', 'duerme', 'cama']\n",
      "3: ['perros', 'juegan', 'parque']\n",
      "4: ['perro', 'duerme', 'cama', 'duerme', 'sillón']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('spanish'))\n",
    "tokens_no_stopwords = [[word for word in doc if word not in stop_words] for doc in tokens]\n",
    "\n",
    "print(\"\\nTokens sin stopwords:\")\n",
    "for i, tks in enumerate(tokens_no_stopwords, 1):\n",
    "    print(f\"{i+1}: {tks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520704d",
   "metadata": {},
   "source": [
    "## Stemming (Porter y Snowball)(Español)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "790bd199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming Porter:\n",
      "['gato', 'duerm', 'cama']\n",
      "['perro', 'juegan', 'parqu']\n",
      "['perro', 'duerm', 'cama', 'duerm', 'sillón']\n",
      "Stemming Snowball:\n",
      "['gat', 'duerm', 'cam']\n",
      "['perr', 'jueg', 'parqu']\n",
      "['perr', 'duerm', 'cam', 'duerm', 'sillon']\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"spanish\")\n",
    "\n",
    "print(\"Stemming Porter:\")\n",
    "for tks in tokens_no_stopwords:\n",
    "    print([porter.stem(word) for word in tks])\n",
    "\n",
    "print(\"Stemming Snowball:\")\n",
    "for tks in tokens_no_stopwords:\n",
    "    print([snowball.stem(word) for word in tks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f13ec",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "077bdd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization:(Inglés)\n",
      "Original: ['playing', 'played', 'plays', 'dogs', 'cats', 'running']\n",
      "Lemmatized: ['playing', 'played', 'play', 'dog', 'cat', 'running']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"Lemmatization:(Inglés)\")\n",
    "example_tokens = nltk.word_tokenize(\"playing played plays dogs cats running\", language=\"english\")\n",
    "print(\"Original:\", example_tokens)\n",
    "print(\"Lemmatized:\", [lemmatizer.lemmatize(w) for w in example_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f47c77",
   "metadata": {},
   "source": [
    "## BoW (Bolsa de palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb536f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de características (BoW):\n",
      "   cama  duerme  el  en  gato  juegan  la  los  no  parque  perro  perros  \\\n",
      "0     1       1   1   1     1       0   1    0   0       0      0       0   \n",
      "1     0       0   1   1     0       1   0    1   0       1      0       1   \n",
      "2     1       2   1   2     0       0   2    0   1       0      1       0   \n",
      "\n",
      "   sillón  \n",
      "0       0  \n",
      "1       0  \n",
      "2       1  \n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(normalized_corpus)\n",
    "\n",
    "print(\"\\nMatriz de características (BoW):\")\n",
    "df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df_bow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
